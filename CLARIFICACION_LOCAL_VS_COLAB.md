# ğŸ”„ CLARIFICACIÃ“N: DIVISIÃ“N LOCAL â†” COLAB

**Documento de SeparaciÃ³n ArquitectÃ³nica**

---

## ğŸ“ DISTRIBUCIÃ“N GEOGRÃFICA DEL SISTEMA

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  â”‚         â”‚                                  â”‚
â”‚     LOCAL (Este Workspace)       â”‚         â”‚     COLAB (Google Servers)      â”‚
â”‚                                  â”‚         â”‚                                  â”‚
â”‚  /workspaces/HIPERGRAFO/         â”‚         â”‚  Servidor Python (server.py)     â”‚
â”‚                                  â”‚         â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ  LO QUE OCURRE LOCALMENTE

### 1. **CAPA 0: Entrada (Vector 256D)**

**Archivo**: `src/control/MapeoVector256DaDendritas.ts`

```typescript
class MapeoVector256DaDendritas {
    extraerCamposDendriticos(vector: Vector256D): Omega21Dendrites {
        // D001-D056 extraÃ­dos aquÃ­
        // D057-D256 disponibles para otras funciones
        return dendrites;  // ConfiguraciÃ³n para Simulador
    }
}
```

**Â¿QuÃ© ocurre?**
- âœ… Vector 256D llega como entrada
- âœ… Se extraen 56 campos dendrÃ­ticos
- âœ… Se mapean a sub-redes S1-S25
- âœ… Se prepara configuraciÃ³n

**Â¿DÃ³nde ocurre?**
- ğŸ“ Local (TypeScript/Node.js)

**Â¿CuÃ¡ndo ocurre?**
- â° Cada ciclo de `procesarFlujo()`

---

### 2. **CAPA 1: Sensorial (25 Ãtomos)**

**Archivos**: 
- `src/SistemaOmnisciente.ts` (orquestaciÃ³n)
- `src/hardware/Simulador.ts` (generaciÃ³n de telemetrÃ­a)
- `src/neural/InferenciaLocal.ts` (ONNX)
- `models/omega21_brain.onnx` (modelo pre-entrenado)

```
Para cada ciclo:
    
    1. Vector 256D entra
    2. Dendritas se extraen (56 valores)
    3. Para cada Ãtomo S1-S25:
        
        a) Simulador.configurarDendritas(D001-D056)
           â””â”€ Modifica comportamiento ONNX
        
        b) Simulador.generarMuestra()
           â””â”€ Crea telemetrÃ­a estabilizada
        
        c) InferenciaLocal.predecir(ONNX)
           â””â”€ Ejecuta 1024 neuronas LIF
           â””â”€ Output: embedding 256D
        
        d) AnÃ¡lisis fÃ­sico
           â””â”€ Calcula mÃ©tricas
    
    4. Los 25 embeddings se concatenan â†’ 1600D
    5. Se registra experiencia en EntrenadorCognitivo
```

**Â¿QuÃ© ocurre?**
- âœ… Cada Ãtomo procesa en paralelo
- âœ… Modelo ONNX (1024 LIF) se ejecuta 25 veces
- âœ… Salidas se capturan (256D cada una)
- âœ… Protocolo de InfecciÃ³n propaga anomalÃ­as

**Â¿DÃ³nde ocurre?**
- ğŸ“ Local (TypeScript/Node.js + ONNX Runtime)

**Â¿CuÃ¡ndo ocurre?**
- â° Cada ciclo (configurable, tÃ­picamente cada 100ms)

---

### 3. **CONSOLIDACIÃ“N COGNITIVA (4 Fases)**

**Archivo**: `src/neural/EntrenadorCognitivo.ts`

```
Fase 1: ADQUISICIÃ“N
â”œâ”€ registrarExperiencia()
â”œâ”€ Almacena percepciones + hipergrafo + anomalÃ­a
â””â”€ Buffer: max 50 experiencias

Fase 2: CATEGORIZACIÃ“N  
â”œâ”€ refinarCategorias()
â”œâ”€ Crea Nodos concepto en Hipergrafo
â””â”€ Calcula centroides de percepciones

Fase 3: CONSOLIDACIÃ“N
â”œâ”€ reforzarCausalidad()
â”œâ”€ Crea Hiperedges entre conceptos
â””â”€ Peso inicial: 0.7

Fase 4: PODA
â”œâ”€ podarMemoriaDebil()
â”œâ”€ Elimina edges con weight < 0.1
â””â”€ Mantiene solo conexiones fuertes
```

**Â¿QuÃ© ocurre?**
- âœ… Experiencias se capturan continuamente
- âœ… Cada 50 experiencias: consolidaciÃ³n
- âœ… Conceptos abstractos emergen
- âœ… Relaciones causales se refuerzan
- âœ… Memoria dÃ©bil se poda

**Â¿DÃ³nde ocurre?**
- ğŸ“ Local (TypeScript/Node.js)

**Â¿CuÃ¡ndo ocurre?**
- â° Continuo (Fase 1), cada 50 experiencias (Fases 2-4)

---

### 4. **EXPANSIÃ“N DIMENSIONAL**

**Archivo**: `src/SistemaOmnisciente.ts`

```typescript
expandirAVector1600D(embedding256D: number[]): number[] {
    // Entrada: 256D (salida de un Ãtomo)
    // Proceso: RepeticiÃ³n + modulaciÃ³n harmÃ³nica
    // Salida: 1600D (25 subespacios Ã— 64D)
    
    for (let s = 0; s < 25; s++) {
        for (let i = 0; i < 64; i++) {
            const modulacion = sin((s+1)*Ï€/25) * cos((i+1)*Ï€/64);
            const valor = embedding[i] * (1 + modulacion * 0.3);
            vector1600D.push(valor);
        }
    }
    return vector1600D;
}
```

**Â¿QuÃ© ocurre?**
- âœ… Vector 256D se expande a 1600D
- âœ… Cada subespacio obtiene modulaciÃ³n Ãºnica
- âœ… Resultado: coherencia armÃ³nica

**Â¿DÃ³nde ocurre?**
- ğŸ“ Local (TypeScript/Node.js)

**Â¿CuÃ¡ndo ocurre?**
- â° DespuÃ©s de procesar cada Ãtomo

---

### 5. **STREAMING A COLAB**

**Archivo**: `src/neural/StreamingBridge.ts`

```typescript
async enviarVector(vector1600D: number[], esAnomalia: boolean) {
    // Bufferiza 64 vectores
    // EnvÃ­a batch HTTP POST a Colab
    
    const payload = {
        samples: [
            {
                input_data: vector1600D,    // 1600D
                anomaly_label: esAnomalia ? 1 : 0
            },
            // ... mÃ¡s muestras
        ]
    };
    
    await fetch(`${url}/train_layer2`, {
        method: 'POST',
        headers: { 'Authorization': token },
        body: JSON.stringify(payload)
    });
}
```

**Â¿QuÃ© ocurre?**
- âœ… Vectores 1600D se acumulan
- âœ… Cada 64 muestras: envÃ­o a Colab
- âœ… Etiqueta de anomalÃ­a incluida

**Â¿DÃ³nde ocurre?**
- ğŸ“ Local (TypeScript/Node.js) â†’ ğŸŒ Internet â†’ â˜ï¸ Colab

**Â¿CuÃ¡ndo ocurre?**
- â° Cada 64 ciclos (o por demanda)

---

## â˜ï¸ LO QUE OCURRE EN COLAB

### 1. **CAPA 2: Procesamiento Dual (Temporal + Espacial)**

**Archivo**: `src/colab/server.py` (que se ejecuta en servidor Colab)

```python
class CortezaCognitivaV2(Model):
    
    # CAPA 2A: Temporal (Bi-LSTM)
    lstm_fw = LSTM(256, return_sequences=True)
    lstm_bw = LSTM(256, return_sequences=True, go_backwards=True)
    
    # CAPA 2B: Espacial (Transformer)
    transformer = MultiHeadAttention(
        num_heads=8,
        key_dim=64,
        value_dim=64
    )
    
    def temporal_stream(self, x):
        # x: [batch, 1600D]
        # Procesa secuencias de 128 timesteps
        # Output: [batch, 512D]
        lstm_out = concatenate([
            lstm_fw(x),
            lstm_bw(x)
        ])
        return lstm_out  # 512D
    
    def spatial_stream(self, x):
        # x: [batch, 25, 64D] (reshapear 1600D)
        # Self-attention entre subespacios
        # Output: [batch, 512D]
        attn_out = transformer(x)
        return global_average_pooling(attn_out)  # 512D
```

**Â¿QuÃ© ocurre?**
- âœ… Entrada: 1600D (25 subespacios Ã— 64D)
- âœ… Bi-LSTM procesa secuencias temporales â†’ 512D
- âœ… Transformer procesa correlaciones espaciales â†’ 512D
- âœ… Ambas salidas se concatenan â†’ 1024D

**Â¿DÃ³nde ocurre?**
- â˜ï¸ Colab (Python/TensorFlow/Keras)

**Â¿CuÃ¡ndo ocurre?**
- â° Cuando se recibe batch (cada 64 muestras)

---

### 2. **CAPA 3: Asociativa Inferior (FusiÃ³n)**

```python
# FusiÃ³n inteligente con GMU (Gated Multimodal Unit)
class GatedMultimodalUnit(Layer):
    def __init__(self, units):
        self.units = units
        # Gating mechanism
        self.gate_dense = Dense(1, activation='sigmoid')
    
    def call(self, temporal, spatial):
        # temporal: [batch, 512D]
        # spatial: [batch, 512D]
        
        concatenated = concatenate([temporal, spatial])  # 1024D
        lambda_gate = self.gate_dense(concatenated)
        
        # Weighted fusion
        fused = lambda_gate * temporal + (1 - lambda_gate) * spatial
        
        # MLP Residual
        x = Dense(4096, activation='gelu')(fused)
        x = Dense(4096, activation='gelu')(x)
        x = Dense(4096, activation='gelu')(x)
        
        # Skip connection
        output = Add()([x, Dense(4096)(fused)])
        return output  # 4096D â†’ redimensionar a 1024D
```

**Â¿QuÃ© ocurre?**
- âœ… GMU combina temporal + espacial inteligentemente
- âœ… MLP Residual aprende patrones complejos
- âœ… Output: 1024D (representaciÃ³n unificada)

**Â¿DÃ³nde ocurre?**
- â˜ï¸ Colab (Python/TensorFlow)

**Â¿CuÃ¡ndo ocurre?**
- â° Durante entrenamiento de batch

---

### 3. **CAPA 4: Asociativa Superior (AbstracciÃ³n)**

```python
# Self-Attention para crear conceptos
abstraction = MultiHeadAttention(
    num_heads=16,
    key_dim=64
)(Dense(1024)(fusion_output))

concepts = Dense(256)(abstraction)  # RepresentaciÃ³n de conceptos
```

**Â¿QuÃ© ocurre?**
- âœ… Self-Attention crea representaciones abstractas
- âœ… Output: 256D (vector de conceptos)

**Â¿DÃ³nde ocurre?**
- â˜ï¸ Colab (Python/TensorFlow)

---

### 4. **CAPA 5: Ejecutiva (Meta-CogniciÃ³n)**

```python
# Decisiones ejecutivas
decision_head = Sequential([
    Dense(256, activation='gelu'),
    Dense(128, activation='gelu'),
    Dense(1)  # PredicciÃ³n de anomalÃ­a
])

# TambiÃ©n genera sugerencias
suggestions_head = Dense(16)(concepts)  # 16 ajustes dendrÃ­ticos

outputs = {
    'loss': mse(y_true, y_pred),
    'avg_anomaly_prob': sigmoid(decision_head(concepts)),
    'suggested_adjustments': suggestions_head
}
```

**Â¿QuÃ© ocurre?**
- âœ… Predice si es anomalÃ­a
- âœ… Sugiere ajustes dendrÃ­ticos (16D)
- âœ… Calcula loss para backprop

**Â¿DÃ³nde ocurre?**
- â˜ï¸ Colab (Python/TensorFlow)

---

### 5. **ENTRENAMIENTO**

```python
@app.post('/train_layer2')
async def train_layer2(request: TrainingRequest):
    # Recibe batch de muestras
    # samples = [{'input_data': 1600D[], 'anomaly_label': 0|1}, ...]
    
    # Forward pass
    predictions = model(request.samples['input_data'])
    
    # Backprop
    loss = compute_loss(predictions, request.samples['anomaly_label'])
    optimizer.minimize(loss)
    
    # Retorna feedback
    return {
        'loss': float(loss),
        'avg_anomaly_prob': float(tf.reduce_mean(predictions)),
        'suggested_adjustments': list(suggestions_head.numpy())
    }
```

**Â¿QuÃ© ocurre?**
- âœ… Recibe batch 1600D + etiquetas
- âœ… Forward pass: Capas 2-5
- âœ… Backprop: Actualiza pesos
- âœ… Retorna loss + sugerencias

**Â¿DÃ³nde ocurre?**
- â˜ï¸ Colab (Python/TensorFlow con GPU)

---

## ğŸ”— FEEDBACK LOOP

```
1. LOCAL: Genera vector 1600D
    â†“
2. COLAB: Recibe, procesa, entrena
    â†“
3. COLAB: Retorna suggested_adjustments (16D)
    â†“
4. LOCAL: MapeoVector256DaDendritas aplica â†’ D001-D056
    â†“
5. LOCAL: Siguiente ciclo usa nuevas dendritas
    â†“
[Vuelta a paso 1]
```

---

## ğŸ“Š TABLA RESUMEN

| Componente | Local | Colab | Lenguaje | GPU Requerida |
|-----------|-------|-------|----------|---------------|
| **Capa 0** | âœ… | - | TypeScript | âŒ |
| **Capa 1** | âœ… | - | TypeScript | âš ï¸ (ONNX) |
| **Cognitivo** | âœ… | - | TypeScript | âŒ |
| **ExpansiÃ³n** | âœ… | - | TypeScript | âŒ |
| **Capa 2A** | - | âœ… | Python | âœ… |
| **Capa 2B** | - | âœ… | Python | âœ… |
| **GMU** | - | âœ… | Python | âœ… |
| **Capa 3** | - | âœ… | Python | âœ… |
| **Capa 4** | - | âœ… | Python | âœ… |
| **Capa 5** | - | âœ… | Python | âœ… |
| **Streaming** | âœ… | âœ… | TypeScript/Python | âŒ |

---

## âš¡ FLUJO COMPLETO EN UN CICLO

```
CICLO N:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LOCAL:
  1. generarVectorEntrada256D()
  2. MapeoVector256DaDendritas.extraer(D001-D056)
  3. Para cada Ãtomo S1-S25:
     - Simulador.configurarDendritas(D001-D056)
     - Simulador.generarMuestra()
     - InferenciaLocal.predecir(ONNX)  â† AQUÃ SE ENTRENA EL ONNX
     - Output: embedding 256D
  4. EntrenadorCognitivo.registrarExperiencia()
  5. expandirAVector1600D(256D) â†’ 1600D
  6. StreamingBridge.bufferizar(1600D)
  
  [Si buffer = 64]:
    7. StreamingBridge.enviarVector(batch_1600D)
    
COLAB:
  1. POST /train_layer2 recibe batch
  2. Capa 2A (Bi-LSTM) procesa temporal
  3. Capa 2B (Transformer) procesa espacial
  4. Capa 3 (GMU + MLP) fusiona
  5. Capa 4 (Attention) abstrae
  6. Capa 5 (Decision) predice
  7. Backprop: Actualiza Capas 2-5
  8. POST Response retorna:
     - loss
     - avg_anomaly_prob
     - suggested_adjustments (16D)
  
LOCAL (Siguiente Ciclo):
  1. Recibe suggested_adjustments
  2. Actualiza D001-D056
  3. Vuelve a paso 1
```

---

## ğŸ¯ CONCLUSIÃ“N

**LOCAL (SistemaOmnisciente):**
- âœ… Procesa datos de entrada
- âœ… Ejecuta 25 Ãtomos ONNX
- âœ… Consolida cognitivamente
- âœ… Expande a 1600D
- âœ… EnvÃ­a a Colab

**COLAB (server.py):**
- âœ… Recibe 1600D
- âœ… Entrena Capas 2-5
- âœ… Retorna feedback

**SEPARACIÃ“N CLARA:**
- ğŸ“ **Local = Capas 0-1 + Cognitivo**
- â˜ï¸ **Colab = Capas 2-5 + Entrenamiento**

**Â¿QUÃ‰ SE ENTRENA?**
- ğŸ”´ **LOCAL**: ONNX omega21_brain.onnx (ya pre-entrenado)
- ğŸ”´ **LOCAL**: Entrenador Cognitivo (consolidaciÃ³n de memoria)
- ğŸ”µ **COLAB**: Capas 2-5 (el grueso del aprendizaje)

---

*ClarificaciÃ³n de Arquitectura - 23 de Diciembre de 2025*
