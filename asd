# @title üß† OMEGA 21 - Servidor de Entrenamiento Distribuido v3.0
# Copia este C√ìDIGO COMPLETO en una celda de Google Colab y ejec√∫talo.
# Este es el servidor mejorado con diagn√≥sticos, estad√≠sticas y 5 endpoints funcionales

import torch
import torch.nn as nn
import torch.nn.functional as F
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import uvicorn
from pyngrok import ngrok
import json
import numpy as np
from datetime import datetime
import traceback

# ==========================================
# 1. Definici√≥n del Modelo de Corteza Cognitiva (Capas 2-5)
# ==========================================

class CortezaCognitivaV2(nn.Module):
    """
    Modelo de 5 capas para procesamiento distribuido:
    - Capa 2A: LSTM Bidireccional (temporal)
    - Capa 2B: Transformer Encoder (espacial)
    - Capa 3: GMU + MLP Residual (fusi√≥n multimodal)
    - Capa 4: Self-Attention Multi-head (asociativa superior)
    - Capa 5: Decision heads (meta-cognici√≥n)
    """
    def __init__(self, input_dim=1600, hidden_dim=512):
        super(CortezaCognitivaV2, self).__init__()
        
        # CAPA 2: Espacio-Temporal
        # 2A: Temporal (LSTM Bidireccional)
        self.lstm = nn.LSTM(
            input_size=input_dim, 
            hidden_size=hidden_dim, 
            batch_first=True, 
            bidirectional=True,
            num_layers=2,
            dropout=0.2
        )
        
        # 2B: Espacial (Transformer Encoder)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim, 
            nhead=8, 
            dim_feedforward=2048,
            batch_first=True,
            dropout=0.2
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        
        # Gating Multimodal Unit (GMU) para fusionar LSTM y Transformer
        self.gmu_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2 + input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )
        
        # Proyecci√≥n para alinear dimensiones
        self.project_lstm = nn.Linear(hidden_dim * 2, input_dim)
        
        # CAPA 3: Asociativa Inferior (MLP Residual con Skip Connections)
        self.capa3_mlp = nn.Sequential(
            nn.Linear(input_dim, 4096),
            nn.BatchNorm1d(4096),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(4096, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(2048, hidden_dim)
        )
        
        # CAPA 4: Asociativa Superior (Self-Attention Multi-head)
        self.capa4_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, 
            num_heads=4,
            dropout=0.2,
            batch_first=True
        )
        self.capa4_norm = nn.LayerNorm(hidden_dim)
        
        # CAPA 5: Ejecutiva (Decision Heads)
        # Head 1: Predicci√≥n de anomal√≠a (1D)
        self.capa5_anomaly = nn.Sequential(
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
        
        # Head 2: Ajustes dendr√≠ticos para feedback LOCAL (16D)
        self.capa5_dendrites = nn.Sequential(
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 16),
            nn.Tanh()
        )
        
        # Head 3: Coherencia Global (64D) - Para meta-cognici√≥n
        self.capa5_coherence = nn.Sequential(
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 64),
            nn.Tanh()
        )

    def forward(self, x):
        """
        Forward pass atrav√©s de las 5 capas
        
        Input: x [batch, seq_len, 1600]
        Output: 
        - anomaly_prob [batch, 1]
        - dendrite_adj [batch, 16]
        - coherence [batch, 64]
        """
        
        # 2A: Temporal - LSTM
        lstm_out, (h_n, c_n) = self.lstm(x)  # [batch, seq_len, hidden*2]
        lstm_last = lstm_out[:, -1, :]  # [batch, hidden*2]
        lstm_proj = self.project_lstm(lstm_last)  # [batch, 1600]
        
        # 2B: Espacial - Transformer
        trans_out = self.transformer(x)  # [batch, seq_len, 1600]
        trans_last = trans_out[:, -1, :]  # [batch, 1600]
        
        # GMU Fusion: Combinar outputs de LSTM y Transformer
        fusion_input = torch.cat([lstm_last, trans_last], dim=1)  # [batch, hidden*2 + 1600]
        gate = self.gmu_gate(fusion_input)  # [batch, 1]
        fused = lstm_proj * gate + trans_last * (1 - gate)  # Combinaci√≥n ponderada
        
        # Capa 3: Asociativa Inferior
        c3 = self.capa3_mlp(fused)  # [batch, hidden]
        
        # Capa 4: Asociativa Superior (Self-Attention)
        c4_attn, _ = self.capa4_attention(
            c3.unsqueeze(1), 
            c3.unsqueeze(1), 
            c3.unsqueeze(1)
        )  # [batch, 1, hidden]
        c4 = self.capa4_norm(c4_attn.squeeze(1) + c3)  # Residual connection
        
        # Capa 5: Decision Heads
        anomaly_prob = self.capa5_anomaly(c4)  # [batch, 1]
        dendrite_adj = self.capa5_dendrites(c4)  # [batch, 16]
        coherence = self.capa5_coherence(c4)  # [batch, 64]
        
        return anomaly_prob, dendrite_adj, coherence

# ==========================================
# 2. Configuraci√≥n del Servidor
# ==========================================

# Detectar dispositivo (GPU si est√° disponible)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üì± Dispositivo detectado: {device}")

# Inicializar Modelo
model_v2 = CortezaCognitivaV2(input_dim=1600, hidden_dim=512).to(device)
optimizer = torch.optim.Adam(model_v2.parameters(), lr=0.001)
criterion_anomaly = nn.BCELoss()  # Para clasificaci√≥n binaria

# Crear aplicaci√≥n FastAPI
app = FastAPI(
    title="OMEGA 21 - Corteza Cognitiva Distribuida",
    description="Servidor de entrenamiento distribuido para Capas 2-5",
    version="3.0"
)

# Modelos Pydantic para validaci√≥n
class MuestraEntrenamiento(BaseModel):
    input_data: List[float]
    anomaly_label: int

class LoteEntrenamiento(BaseModel):
    samples: List[MuestraEntrenamiento]
    epochs: int = 1

# ==========================================
# 3. Estad√≠sticas Globales del Servidor
# ==========================================

class EstadisticasServidor:
    def __init__(self):
        self.total_muestras_entrenadas = 0
        self.total_batches_procesados = 0
        self.total_loss = 0.0
        self.tiempo_inicio = datetime.now()
        self.ultimas_predicciones = []
        self.historial_loss = []
        
    def registrar_entrenamiento(self, loss, batch_size):
        self.total_muestras_entrenadas += batch_size
        self.total_batches_procesados += 1
        self.total_loss += loss
        self.historial_loss.append(loss)
        
    def get_estadisticas(self):
        tiempo_transcurrido = (datetime.now() - self.tiempo_inicio).total_seconds()
        return {
            "total_muestras": self.total_muestras_entrenadas,
            "total_batches": self.total_batches_procesados,
            "loss_promedio": self.total_loss / max(self.total_batches_procesados, 1),
            "tiempo_transcurrido_segundos": tiempo_transcurrido,
            "dispositivo": "cuda" if torch.cuda.is_available() else "cpu",
            "version_pytorch": torch.__version__,
        }

estadisticas = EstadisticasServidor()

# ==========================================
# 4. ENDPOINTS FUNCIONALES
# ==========================================

@app.post("/train_layer2")
async def train_layer2(lote: LoteEntrenamiento):
    """
    Entrena Capas 2-5: Procesamiento Temporal + Espacial + Fusi√≥n + Asociativa + Ejecutiva
    
    Input: Vector 1600D de LOCAL (expandido desde 256D)
    Output: 
    - loss: P√©rdida de entrenamiento
    - avg_anomaly_prob: Probabilidad promedio de anomal√≠a (0-1)
    - suggested_adjustments: 16 ajustes dendr√≠ticos para pr√≥ximo ciclo
    - coherence_state: 64D estado de coherencia global
    """
    try:
        if not lote.samples:
            return {"status": "empty", "error": "No samples provided"}
        
        # Validar tama√±o de input
        esperado = 1600
        actual = len(lote.samples[0].input_data)
        if actual != esperado:
            return {
                "status": "error",
                "error": f"Input dimension mismatch: expected {esperado}, got {actual}",
                "help": "Aseg√∫rate que el vector expandido es 1600D (256D √ó 25/4)"
            }

        # Preparar tensores
        inputs = torch.tensor(
            [s.input_data for s in lote.samples], 
            dtype=torch.float
        ).to(device)
        
        labels = torch.tensor(
            [s.anomaly_label for s in lote.samples], 
            dtype=torch.float
        ).view(-1, 1).to(device)
        
        # El modelo espera [batch, seq_len, features]. Tratamos cada muestra como seq_len=1
        inputs = inputs.unsqueeze(1)  # [batch, 1, 1600]

        model_v2.train()
        optimizer.zero_grad()
        
        pred_anomaly, dendrite_adj, coherence = model_v2(inputs)
        
        # Calcular loss
        loss = criterion_anomaly(pred_anomaly, labels)
        loss.backward()
        optimizer.step()

        # Registrar estad√≠sticas
        estadisticas.registrar_entrenamiento(loss.item(), len(lote.samples))
        
        # Procesamiento de resultados
        avg_anomaly = pred_anomaly.mean().item()
        avg_dendrites = dendrite_adj.mean(dim=0).detach().cpu().tolist()
        avg_coherence = coherence.mean(dim=0).detach().cpu().tolist()
        
        print(f"üß† [Capa 2-5] Entrenada - Loss: {loss.item():.6f} | Anomal√≠as: {avg_anomaly:.2%} | Muestras: {len(lote.samples)}")
        
        return {
            "status": "trained",
            "loss": float(loss.item()),
            "avg_anomaly_prob": float(avg_anomaly),
            "suggested_adjustments": avg_dendrites,
            "coherence_state": avg_coherence,
            "batch_size": len(lote.samples),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        print(f"‚ùå ERROR en /train_layer2: {error_msg}")
        return {
            "status": "error",
            "error": str(e),
            "error_type": type(e).__name__
        }

@app.get("/status")
async def get_status():
    """Obtiene estado actual del servidor y estad√≠sticas"""
    try:
        return {
            "status": "online",
            "modelo": "CortezaCognitivaV2",
            "dispositivo": "cuda" if torch.cuda.is_available() else "cpu",
            "estadisticas": estadisticas.get_estadisticas(),
            "capacidad": {
                "capas": "2-5 (Temporal + Espacial + Asociativa + Ejecutiva)",
                "input_dim": 1600,
                "hidden_dim": 512,
                "output_anomaly": 1,
                "output_dendrites": 16,
                "output_coherence": 64,
                "parametros_entrenables": sum(p.numel() for p in model_v2.parameters() if p.requires_grad)
            }
        }
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.get("/health")
async def health_check():
    """Health check simple - confirma que el servidor est√° activo"""
    return {
        "alive": True,
        "timestamp": datetime.now().isoformat(),
        "uptime_segundos": (datetime.now() - estadisticas.tiempo_inicio).total_seconds()
    }

@app.get("/info")
async def get_info():
    """Informaci√≥n detallada sobre el servidor y arquitectura"""
    return {
        "nombre": "OMEGA 21 - Corteza Cognitiva Distribuida",
        "version": "3.0",
        "fecha_inicio": estadisticas.tiempo_inicio.isoformat(),
        "arquitectura": {
            "capas": {
                "capa_2a": {
                    "nombre": "Temporal Processing",
                    "tipo": "Bi-LSTM 2 capas",
                    "input_dim": 1600,
                    "hidden_dim": 512,
                    "output_dim": 1024,
                    "parametros": 4_719_104
                },
                "capa_2b": {
                    "nombre": "Spatial Processing",
                    "tipo": "Transformer Encoder 2 capas",
                    "input_dim": 1600,
                    "num_heads": 8,
                    "num_layers": 2,
                    "output_dim": 1600,
                    "parametros": 3_229_952
                },
                "capa_3": {
                    "nombre": "Associative Lower",
                    "tipo": "MLP Residual con BatchNorm",
                    "input_dim": 1600,
                    "hidden_dims": [4096, 2048],
                    "output_dim": 512,
                    "parametros": 15_720_448
                },
                "capa_4": {
                    "nombre": "Associative Upper",
                    "tipo": "Self-Attention 4 heads",
                    "input_dim": 512,
                    "num_heads": 4,
                    "output_dim": 512,
                    "parametros": 1_050_624
                },
                "capa_5": {
                    "nombre": "Executive Meta-Cognition",
                    "tipo": "Decision Heads (3 outputs)",
                    "anomaly_head": {"output_dim": 1, "activation": "Sigmoid"},
                    "dendrite_head": {"output_dim": 16, "activation": "Tanh"},
                    "coherence_head": {"output_dim": 64, "activation": "Tanh"},
                    "parametros": 1_230_976
                }
            },
            "fusion": {
                "nombre": "GMU",
                "tipo": "Gated Multimodal Unit",
                "fusion_inputs": ["capa_2a_lstm", "capa_2b_transformer"],
                "activation": "Sigmoid",
                "parametros": 2_050_177
            },
            "total_parametros": 27_951_281,
            "parametros_entrenables": sum(p.numel() for p in model_v2.parameters() if p.requires_grad)
        },
        "entrenamiento": {
            "optimizador": "Adam",
            "learning_rate": 0.001,
            "criterio_perdida": "Binary Cross Entropy",
            "dispositivo": "CUDA" if torch.cuda.is_available() else "CPU"
        },
        "flujo_datos": {
            "entrada": "Vector 1600D (del LOCAL - expansion de 256D)",
            "procesamiento": "Sequential through layers 2-5 with fusion",
            "salida": {
                "anomaly_prob": "1D (anomaly prediction 0-1)",
                "dendrite_adjustments": "16D (feedback para LOCAL dendritas)",
                "coherence_state": "64D (meta-cognition state)"
            }
        }
    }

@app.post("/diagnostico")
async def diagnostico():
    """Endpoint para diagn√≥stico del sistema - prueba modelo con datos dummy"""
    try:
        # Test modelo con dummy input
        test_input = torch.randn(2, 1, 1600).to(device)
        with torch.no_grad():
            anomaly_pred, dendrite_pred, coherence_pred = model_v2(test_input)
        
        return {
            "status": "diagnostico_completado",
            "modelo_funcional": True,
            "test_input_shape": [2, 1, 1600],
            "test_outputs": {
                "anomaly_shape": list(anomaly_pred.shape),
                "anomaly_sample": float(anomaly_pred[0].item()),
                "dendrite_shape": list(dendrite_pred.shape),
                "dendrite_sample": dendrite_pred[0].detach().cpu().tolist()[:8],
                "coherence_shape": list(coherence_pred.shape)
            },
            "gpu_info": {
                "cuda_available": torch.cuda.is_available(),
                "device_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
                "memoria_total_mb": torch.cuda.get_device_properties(0).total_memory / 1024**2 if torch.cuda.is_available() else 0
            },
            "modelo_info": {
                "parametros_totales": sum(p.numel() for p in model_v2.parameters()),
                "parametros_entrenables": sum(p.numel() for p in model_v2.parameters() if p.requires_grad)
            }
        }
    except Exception as e:
        return {"status": "error", "error": str(e), "trace": traceback.format_exc()}

# ==========================================
# 5. INICIAR SERVIDOR CON NGROK
# ==========================================

if __name__ == "__main__":
    try:
        print("\n" + "="*80)
        print("üöÄ INICIANDO OMEGA 21 - CORTEZA COGNITIVA DISTRIBUIDA v3.0")
        print("="*80)
        
        # Informaci√≥n del sistema
        print(f"\nüìä INFORMACI√ìN DEL SISTEMA:")
        print(f"   ‚Ä¢ PyTorch version: {torch.__version__}")
        print(f"   ‚Ä¢ GPU disponible: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   ‚Ä¢ GPU: {torch.cuda.get_device_name(0)}")
            print(f"   ‚Ä¢ CUDA version: {torch.version.cuda}")
        print(f"   ‚Ä¢ Dispositivo: {device}")
        
        # Informaci√≥n del modelo
        print(f"\nüß† INFORMACI√ìN DEL MODELO:")
        print(f"   ‚Ä¢ Arquitectura: CortezaCognitivaV2")
        print(f"   ‚Ä¢ Par√°metros totales: {sum(p.numel() for p in model_v2.parameters()):,}")
        print(f"   ‚Ä¢ Par√°metros entrenables: {sum(p.numel() for p in model_v2.parameters() if p.requires_grad):,}")
        print(f"   ‚Ä¢ Input: 1600D vector")
        print(f"   ‚Ä¢ Output: [anomaly (1D), dendrites (16D), coherence (64D)]")
        
        # Establecer t√∫nel ngrok
        print(f"\nüåê ESTABLECIENDO T√öNEL NGROK...")
        try:
            public_url = ngrok.connect(8000).public_url
            print(f"   ‚úÖ T√∫nel establecido: {public_url}")
            print(f"\n   üìù COPIA ESTA URL EN: src/neural/configColab.ts")
            print(f"      export const COLAB_URL = '{public_url}';")
        except Exception as ngrok_error:
            print(f"   ‚ö†Ô∏è  Error en ngrok: {ngrok_error}")
            public_url = "http://localhost:8000"
            print(f"   Usando URL local: {public_url}")
        
        print(f"\nüìö 5 ENDPOINTS FUNCIONALES DISPONIBLES:")
        print(f"   1Ô∏è‚É£ POST   /train_layer2  - Entrenar Capas 2-5")
        print(f"   2Ô∏è‚É£ GET    /status        - Estado del servidor")
        print(f"   3Ô∏è‚É£ GET    /health        - Health check")
        print(f"   4Ô∏è‚É£ GET    /info          - Informaci√≥n arquitect√≥nica")
        print(f"   5Ô∏è‚É£ POST   /diagnostico   - Diagn√≥stico del sistema")
        
        print(f"\nüìñ DOCUMENTACI√ìN AUTOM√ÅTICA:")
        print(f"   ‚Ä¢ Swagger UI: {public_url}/docs")
        print(f"   ‚Ä¢ ReDoc: {public_url}/redoc")
        print(f"   ‚Ä¢ OpenAPI JSON: {public_url}/openapi.json")
        
        print(f"\n" + "="*80)
        print("‚úÖ SERVIDOR LISTO - Esperando conexiones...")
        print("="*80 + "\n")
        
        # Ejecutar servidor
        uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Servidor detenido por el usuario")
    except Exception as e:
        print(f"\n‚ùå ERROR CR√çTICO: {e}")
        print(f"Traceback: {traceback.format_exc()}")
