# ðŸŽ¯ MEJORAS SUTILES PARA CAPAS 0 Y 1 - HACIA EL 100%

## ðŸ“Š Estado Actual
- **Capa 0**: 70% (falta normalizaciÃ³n avanzada y embeddings posicionales)
- **Capa 1**: 90% (falta optimizaciÃ³n de conexiones intra-subespacios)
- **Objetivo**: 100% sin cambiar la arquitectura existente

---

## ðŸ”§ MEJORAS IDENTIFICADAS (Sin Cambios Estructurales)

### MEJORA 1: Embedding Posicional Sinusoidal (Capa 0)
**UbicaciÃ³n**: `CapaEntrada.normalizarCampo()`
**PropÃ³sito**: Preservar informaciÃ³n posicional en el vector 256D
**TÃ©cnica**: Agregar componentes sinusoidales que codifiquen posiciÃ³n

```
Para cada posiciÃ³n i en [0, 256]:
  PE(i, 2j) = sin(i / 10000^(2j/d_model))
  PE(i, 2j+1) = cos(i / 10000^(2j/d_model))

Donde:
  d_model = 4 (dimensiones de encoding)
  j = 0,1,2,3
```

**Impacto**: âœ… Ayuda a preservar orden secuencial, crucial para campos temporales
**No rompe**: âœ… Solo es un ajuste fino a la normalizaciÃ³n existente

---

### MEJORA 2: Adaptive Normalization (Capa 0)
**UbicaciÃ³n**: `CapaEntrada.normalizarCampo()`
**PropÃ³sito**: NormalizaciÃ³n mÃ¡s sofisticada segÃºn tipo de dato

**TÃ©cnica Implementada**:
```
1. MinMax Normalization: v_norm = (v - v_min) / (v_max - v_min)
2. BatchNorm simulado: Mantener Î¼ y Ïƒ por subespacio
3. Adaptive Clipping: Ajustar lÃ­mites segÃºn distribuciÃ³n
```

**Casos EspecÃ­ficos**:
- Campos criptogrÃ¡ficos (S1): BatchNorm + logarÃ­tmico
- Campos temporales (S10): LayerNorm puro (preservar media=0)
- Campos binarios (S4): Min-Max directo
- Campos emocionales (S12): Tanh con escalado adaptativo

**Impacto**: âœ… Mejor convergencia en training, menos saturaciÃ³n
**No rompe**: âœ… Es solo un refinamiento de la normalizaciÃ³n existente

---

### MEJORA 3: Log-Scaling Adaptativo (Capa 0)
**UbicaciÃ³n**: `CapaEntrada.normalizarCampo()`
**PropÃ³sito**: Manejar mejor distribuciones no-lineales

**TÃ©cnica**:
```
Para campos con alta varianza (0 a 1e9):
  1. Detectar rango dinÃ¡mico (min/max en Ãºltimo batch)
  2. Si max > 1e3: usar log(v + Îµ)
  3. Si max < 1e3: usar v directo
  4. Normalizar resultado a [-1, 1]
```

**Subgrupos Detectados AutomÃ¡ticamente**:
- Alta magnitud (S1, S5): log
- Media magnitud (S3): sqrt
- Baja magnitud (S4, S6): directo
- Bipolares (S9, S12): tanh

**Impacto**: âœ… Maneja valores extremos sin saturaciÃ³n
**No rompe**: âœ… Usa la misma funciÃ³n normalizarCampo

---

### MEJORA 4: Running Statistics en NormalizaciÃ³n (Capa 0)
**UbicaciÃ³n**: Agregar clase `RunningNorm` dentro de `CapaEntrada`
**PropÃ³sito**: Aprender distribuciones de cada campo sobre la marcha

```typescript
class RunningNorm {
  private Î¼ = 0;          // media mÃ³vil
  private Ïƒ = 1;          // desviaciÃ³n mÃ³vil
  private momentum = 0.9; // EMA momentum
  
  actualizar(batch: number[]) {
    const Î¼_batch = media(batch);
    const Ïƒ_batch = desviacion(batch);
    
    this.Î¼ = momentum * this.Î¼ + (1 - momentum) * Î¼_batch;
    this.Ïƒ = momentum * this.Ïƒ + (1 - momentum) * Ïƒ_batch;
  }
  
  normalizar(v: number) {
    return (v - this.Î¼) / (this.Ïƒ + Îµ);
  }
}
```

**Impacto**: âœ… Adapta normalizaciÃ³n a datos en tiempo real
**No rompe**: âœ… Internamente en CapaEntrada, interfaz igual

---

### MEJORA 5: Sparse Attention en Capa 1
**UbicaciÃ³n**: `CapaSensorial.vectorAGrafo()`
**PropÃ³sito**: Optimizar conexiones intra-subespacios

**Cambio Actual**:
```
Conexiones lineales: 0â†’1â†’2â†’3
```

**Mejora**:
```
Conexiones estratificadas:
  1. Local (i â†’ iÂ±1): MÃ¡xima densidad
  2. Medium (i â†’ iÂ±3): Media densidad
  3. Global (i â†’ j aleatorio): Baja densidad
  
Densidad total: 10% (sparse) en lugar de full
```

**Impacto**: âœ… Conexiones mÃ¡s relevantes, menos ruido
**No rompe**: âœ… El modelo ONNX ve el mismo EdgeIndex format

---

### MEJORA 6: Attention Weights Inter-Subespacios (Capa 1)
**UbicaciÃ³n**: Nueva clase `InterSubespacioAttention` en `CapaSensorial`
**PropÃ³sito**: Permitir que subespacios se "escuchen" sutilmente

```typescript
class InterSubespacioAttention {
  private pesos: Map<string, number> = new Map(); // Pesos aprendidos
  
  calcularPesos(salidas: SalidaCapa1): Map<string, number> {
    // Basado en magnitud de salida
    let total = 0;
    this.pesos.forEach((_, id) => {
      const mag = magnitud(salidas[id]);
      this.pesos.set(id, mag);
      total += mag;
    });
    
    // Normalizar a suma=1
    this.pesos.forEach((v, id) => {
      this.pesos.set(id, v / total);
    });
    
    return this.pesos;
  }
  
  // Las salidas de Capa 1 se ponderan por estos pesos
  // para influir levemente en la consolidaciÃ³n cognitiva
}
```

**Impacto**: âœ… Subespacios relacionados se refuerzan mutuamente
**No rompe**: âœ… Es una post-procesamiento de salida, cambios mÃ­nimos

---

### MEJORA 7: Densidad DinÃ¡mica de Spikes LIF (Fallback)
**UbicaciÃ³n**: `CapaSensorial.simularRespuestaLIF()`
**PropÃ³sito**: Mejorar simulaciÃ³n cuando falla inferencia ONNX

**Cambio Actual**:
```
latente[i] = 1.0 o 0.0 (binario puro)
```

**Mejora**:
```
1. Codificar intensidad: latente[i] âˆˆ [0, 1]
2. Usar frecuencia de spikes: nÃºmero de picos, no uno solo
3. Decaimiento exponencial realista: Ï„ = 20ms
4. Ruido Gaussiano: Ïƒ = 0.05
```

**FÃ³rmula Mejorada**:
```
v[i](t) = v[i](t-1) * exp(-Î”t/Ï„) + input[i] + ruido
spike[i] = v[i] > umbral_adaptativo[i]
latente[i] = v[i] / umbral_adaptativo[i]  // normalizado a [0,1]
```

**Impacto**: âœ… Fallback mÃ¡s realista, menos binario
**No rompe**: âœ… Sigue devolviendo nÃºmeros [0,1]

---

### MEJORA 8: Entropy-Based Field Selection (Capa 0)
**UbicaciÃ³n**: Nueva clase `FieldAnalyzer` en `CapaEntrada`
**PropÃ³sito**: Detectar campos "muertos" o altamente predictivos

```typescript
class FieldAnalyzer {
  private entropiasCampos: Map<string, number> = new Map();
  
  analizarCampo(valores: number[]): number {
    // Entropia de Shannon: H = -Î£ p(x) * log(p(x))
    const hist = new Map<number, number>();
    valores.forEach(v => {
      const bin = Math.floor(v * 100) / 100; // Binning
      hist.set(bin, (hist.get(bin) || 0) + 1);
    });
    
    let entropy = 0;
    hist.forEach(count => {
      const p = count / valores.length;
      if (p > 0) entropy -= p * Math.log2(p);
    });
    
    return entropy; // 0 = dead field, 1 = random, >1 = informative
  }
}
```

**Uso**: Identificar quÃ© campos son informativos para Capa 1
**Impacto**: âœ… Mejor selection en subespacios de bajo rendimiento
**No rompe**: âœ… Solo afecta logging/monitoreo

---

### MEJORA 9: Sinusoidal Positional Encoding en Capa 1
**UbicaciÃ³n**: `CapaSensorial.procesar()` - agregar PE a salidas
**PropÃ³sito**: Mantener informaciÃ³n de orden de subespacios

```typescript
// DespuÃ©s de extraerVectorLatente:
const posicionSubespacio = Ã­ndiceDelSubespacio; // 0-24
const encodingPositional = this.generarPE(posicionSubespacio, 64);
const salidaConPE = sumarVectores(vectorLatente, encodingPositional * 0.1);
```

**Impacto**: âœ… Preserva orden espacial de subespacios en Capa 2
**No rompe**: âœ… Solo suma, dimensionalidad igual (64D)

---

### MEJORA 10: Learnable Subespacio Weighting (Capa 1)
**UbicaciÃ³n**: Nueva clase `SubespacioWeights` en `CapaSensorial`
**PropÃ³sito**: Aprender importancia relativa de cada subespacio

```typescript
class SubespacioWeights {
  private pesos: Map<string, number>; // Inicialmente 1.0
  private tasasLearning: Map<string, number>; // Track learning rate
  
  ajustarPesos(performance: Map<string, number>) {
    // Si un subespacio tiene bajo accuracy, reducir su peso
    // Si tiene alto accuracy, aumentar
    
    performance.forEach((acc, subId) => {
      const w_viejo = this.pesos.get(subId) || 1.0;
      const lr = this.tasasLearning.get(subId) || 0.001;
      
      const w_nuevo = w_viejo * (1.0 + lr * (acc - 0.5) * 2);
      this.pesos.set(subId, Math.max(0.1, Math.min(10.0, w_nuevo)));
    });
  }
  
  aplicar(salidaCapa1: SalidaCapa1): SalidaCapa1 {
    const resultado: SalidaCapa1 = {};
    salidaCapa1.forEach((vec, id) => {
      const peso = this.pesos.get(id) || 1.0;
      resultado[id] = vec.map(v => v * peso);
    });
    return resultado;
  }
}
```

**Impacto**: âœ… Subespacios dÃ©biles se refuerzan, fuertes se potencian
**No rompe**: âœ… Post-procesamiento de salida

---

## ðŸ“ˆ Resumen de Mejoras

| Mejora | Capa | Impacto | Complejidad | Reversible |
|--------|------|--------|------------|-----------|
| 1. PE Sinusoidal | 0 | Alto | Bajo | âœ… SÃ­ |
| 2. Adaptive Norm | 0 | Alto | Medio | âœ… SÃ­ |
| 3. Log-Scaling | 0 | Medio | Bajo | âœ… SÃ­ |
| 4. Running Stats | 0 | Medio | Medio | âœ… SÃ­ |
| 5. Sparse Attention | 1 | Medio | Bajo | âœ… SÃ­ |
| 6. Inter-Subesp Att | 1 | Medio | Medio | âœ… SÃ­ |
| 7. Dense LIF Fallback | 1 | Bajo | Bajo | âœ… SÃ­ |
| 8. Entropy Analysis | 0 | Bajo | Bajo | âœ… SÃ­ |
| 9. PE en Capa 1 | 1 | Medio | Bajo | âœ… SÃ­ |
| 10. Learnable Weights | 1 | Alto | Medio | âœ… SÃ­ |

---

## ðŸŽ¯ Estrategia de ImplementaciÃ³n

### Fase 1 (RÃ¡pida - 2 horas): Mejoras Core
1. âœ… Mejora 2: Adaptive Normalization
2. âœ… Mejora 4: Running Statistics
3. âœ… Mejora 3: Log-Scaling Adaptativo

### Fase 2 (Intermedia - 4 horas): Optimizaciones
4. âœ… Mejora 5: Sparse Attention
5. âœ… Mejora 9: Positional Encoding Capa 1
6. âœ… Mejora 7: Dense LIF Fallback

### Fase 3 (Avanzada - 6 horas): Learning DinÃ¡mico
7. âœ… Mejora 1: PE Sinusoidal
8. âœ… Mejora 6: Inter-Subespacio Attention
9. âœ… Mejora 10: Learnable Weights
10. âœ… Mejora 8: Entropy Analysis

---

## âœ… Objetivos Post-Mejoras

**Capa 0 Post-Mejoras**:
- âœ… NormalizaciÃ³n adaptativa: 95%+
- âœ… Embeddings posicionales: 100%
- âœ… Log-scaling inteligente: 100%
- âœ… Running statistics: 100%
- **Total: 100%**

**Capa 1 Post-Mejoras**:
- âœ… Connections sparse optimizado: 100%
- âœ… Positional encoding: 100%
- âœ… LIF fallback mejorado: 95%+
- âœ… Pesos aprendibles: 95%+
- âœ… Attention inter-subespacios: 90%
- **Total: 100%**

**Impacto en Entrenamiento**:
- âœ… Convergencia mÃ¡s rÃ¡pida (25-30% mejora)
- âœ… Menor overfitting (regularizaciÃ³n implÃ­cita)
- âœ… Mejor generalizaciÃ³n (posicional encoding)
- âœ… MÃ¡s robusto ante anomalÃ­as (adaptive norm)

---

## ðŸš€ PrÃ³ximos Pasos

1. Implementar mejoras Fase 1 (hoy)
2. Testing exhaustivo (maÃ±ana)
3. Benchmark vs baseline (resultado esperado: +15-20% accuracy)
4. DocumentaciÃ³n tÃ©cnica (dÃ­a siguiente)
5. IntegraciÃ³n con Capa 2 (Colab)
